\documentclass{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}

\newcommand{\Z}{\mathbb{Z}}
\newcommand*{\divmod}[2]{\mathbf{divmod}\left( #1, #2 \right)}
\newcommand{\nmid}{\hspace{-4pt}\not|\hspace{2pt}}
\newtheorem{theorem}{Theorem}

\begin{document}
\title{Application of Discrete Models}
\author{Adam Zlehovszky}
\maketitle

\section{Representation of Integers}

\subsection{Euclidean division}
If $a,b \in \Z$ with $b \ne 0$ then $\exists ! q,r \in \Z$ such that $a=qb+r$ where $0 \le r < |b|$.
This is the \emph{Euclidean division} or \emph{long division} of the \emph{dividend} $a$ with the \emph{divisor} $b$.
The results of the division are the \emph{quotient} $q$ and the \emph{remainder} $r$.
The standard notation for the remainder is $a \bmod b$. In algorithmic setting we use $q, r \gets \divmod{a}{b}$.

\subsection{Number systems}

Let $1 < b \in \Z$ be the \emph{base} of the \emph{number system}.
For each $0 \le n \in \Z$ there exists a unique $1 \le d \in \Z$ and a unique set of \emph{digits} $0 \le n_1, n_2, \ldots, n_{d-1} < b$ all integers, such that
\[
    n = \sum_{k=0}^{d-1}n_k b^{k}.
\]
If $n = 0$, then $d=1$ and $n_0 = 0$. Otherwise $d = \left \lfloor \log_{b} n \right \rfloor + 1$ and we can extract the digits of $n$ with long division, since
\begin{align*}
    n & = n_{d-1}b^{d-1} + \cdots + n_2 b^2 + n_1 b + n_0 \\
      & = \left( n_{d-1}b^{d-2} + \cdots + n_2 b + n_1 \right)b + n_0 
\end{align*}
where the quotient $n_{d-1}b^{d-2} + \cdots + n_2 b + n_1$ is a $d-1$ digit number and $n_0$ is the extracted digit.

We call $n_0$ the \emph{least significant digit} and $n_{d-1}$ the \emph{most significant digit}.
The storage order of digits is called \emph{little endian} if we start at the least significant digits and move towards the most significant one. Otherwise it is called \emph{big endian}.

\subsection{Operations on Integers}

\subsubsection{Addition}

Let us assume that we have two unsigned integers stored as digits in a number system with base $b$:
\[
    n^{(i)} = \sum_{k=0}^{d^{(i)}-1} n_k^{(i)} b^k,
\]
for $i=1,2$.
The following algorithm computes the digits of the sum $s = n^{(1)} + n^{(2)} = \sum_{k=0}^{d^{(s)}-1} s_k b^k$:
\begin{algorithm}
    \caption{Standard addition}
    \label{alg:standard-add}
    \begin{algorithmic}[1]
        \Procedure{StandardAddition}{$n^{(1)}$, $n^{(2)}$}
            \State $d^{(s)} \gets \max\left( d^{(1)}, d^{(2)} \right)$
            \State $c \gets 0$
            \For{$k=0,\ldots,d^{(s)}-1$}
                \State $c,\ s_k \gets \divmod{n_k^{(1)} + n_k^{(2)} + c}{b}$
            \EndFor
            \State \textbf{return} $s$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

In Algorithm \ref{alg:standard-add} we assume that $n^{(i)}_k = 0$ if $k \ge d^{(i)}$ for $i=1,2$.
The time complexity of the standard addition is $O\left(d^{(s)}\right)$.

\subsubsection{Multiplication}

Let $n^{(i)}$'s defined same as above for $i=1,2$.
We will compute the digits of the product $p=n^{(1)} \cdot n^{(2)} = \sum_{k=0}^{d^{(p)}-1} p_k b^k$ with the naive multiplication method:
\begin{algorithm}
    \caption{Naive multiplication}
    \label{alg:naive-mul}
    \begin{algorithmic}[1]
        \Procedure{NaiveMultiplication}{$n^{(1)}$, $n^{(2)}$}
            \State $d^{(p)} \gets d^{(1)} + d^{(2)}$
            \For{$k=0,\ldots,d^{(p)}-1$}
                \State $p_k \gets 0$
            \EndFor
            \For{$j=0,\ldots,d^{(2)}-1$}
                \State $c \gets 0$
                \For{$i=0,\dots,d^{(1)}-1$}
                    \State $c,\ p_{i+j} \gets \divmod{p_{i+j} + n^{(1)}_i n^{(2)}_j + c}{b}$
                \EndFor
            \State $p_{d^{(1)}+j} \gets c$
            \EndFor
            \State \textbf{return} $p$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The time-complexity of Algorithm \ref{alg:naive-mul} is $O(d^{(1)} \cdot d^{(2)})=O(d^2)$, where $d=\max\left( d^{(1)}, d^{(2)}\right)$.

Karatsuba's idea for faster multiplication can be demonstrated on two-digit numbers. Let
\[
    x = x_1 b + x_0,\ \textrm{and}\ y = y_1 b + y_0
\]
with $0 \le x_i, y_i < b$ integers.
Naive multiplication of $x$ and $y$ is
\begin{align*}
    z = xy &= \left(x_1 b + x_0 \right)\left(y_1 b + y_0 \right) \\
           &= x_1 y_1 b^2 + \left(x_1 y_0 + x_0 y_1\right)b + x_0 y_0 \\
           &= z_1 b^2 + z_1 b + z_0.
\end{align*}
This is 4 multiplication and 1 addition.

Now we can express
\begin{align*}
    z_1 &= x_1 y_0 + x_0 y_1 \\
        &= x_1 y_0 + x_0 y_1 - x_1 y_1 + x_1 y_1 - x_0 y_0 + x_0 y_0 \\
        &= \left(x_1 + x_0\right)y_1 + \left(x_1 + x_0\right)y_0 - x_1 y_1 - x_0 y_0 \\
        &= \left(x_1 + x_0\right)\left(y_1 + y_0\right) - x_1 y_1 - x_0 y_0 \\
        &= \left(x_1 + x_0\right)\left(y_1 + y_0\right) - z_2 - z_0.
\end{align*}
This is 3 multiplication and 3 additions.
By extending this idea to more than two digits recursively, the multiplication algorithm performs $O(d^{\log_2 3}) \approx O(d^{1.58})$ single-digit multiplication.

Fast Fourier Transform based algorithms can achieve $O(d \log d)$ complexity.

\subsection{Exponentiation}

We want to compute $x^n$ for some $1 \le n \in \Z$ and $x$ that has multiplication as an operation.

\paragraph{Naive exponentiation}
By repeated multiplication, we can compute
\[
    x^n = \underbrace{x \cdot x \cdots x}_\textrm{$n$ times}.
\]
This method requires $n-1$ multiplications.

\paragraph{Repeated squaring}
If $n=2^s$ for $0 < s \in \Z$, then
\[
    x^{\left(2^s\right)} = \left(x^2\right)^{\left(2^{s-1}\right)}.
\]
This way we can compute $x^n$ with $\log_2 n = s$ multiplications with the algorithm below:
\begin{algorithm}
    \caption{Repeated squaring}
    \label{alg:repeated-squaring}
    \begin{algorithmic}[1]
        \Procedure{RepeatedSquaring}{$x$, $s$}
            \State $y \gets x$
            \For{$k=0,\ldots,s-1$}
                \State $y \gets y^2$
            \EndFor
            \State \textbf{return} y
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\paragraph{Fast exponentiation}

If we write $n = \sum_{k=0}^{d-1}n_k 2^k$ in binary, then
\begin{align*}
    x^n & = x^{\left( \sum_{k=0}^{d-1}n_k 2^k \right)} \\
        & = \prod_{k=0}^{d-1} x^{\left( n_k 2^k \right)} \\
        & = \prod_{k=0}^{d-1} x^{\left( 2^k \right)^{n_k}}.
\end{align*}

Since $y^{n_k} = y$ if $n_k=1$ and $y=1$ otherwise, we arrive at the following algorithm:
\begin{algorithm}
    \caption{Fast exponentiation}
    \label{alg:fast-exp}
    \begin{algorithmic}[1]
        \Procedure{FastExp}{$x$, $n$}
            \State $y \gets 1$
            \While{$n > 0$}
                \State $n, r \gets \divmod{n}{2}$
                \If{$r=1$}
                    \State $y \gets y \cdot x$
                \EndIf
                \State $x \gets x^2$
            \EndWhile
            \State \textbf{return} y
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

This algorithm requires $O(\log_2 n)$ multiplication.

\section{Number Theory and its Applications}

\subsection{Divisibility}

If $aq=b$, then we say that $a$ is a \emph{divisor} of $b$, or $b$ is a \emph{multiple} of $a$.
The notation is $a \mid b$. Otherwise $a \nmid b$.
If $a \mid b$, then long division has remainder of $0$ and in case of integers $\frac{b}{a}$ is an integer as well.

The following properties are natural consequences of the definition.
\begin{enumerate}
    \item For every $a$, we have that $a \mid a$.
    \item $a \mid 0$, for every $a$.
    \item If $0 \mid a$, then $a=0$.
    \item If $a \mid b$ and $b \mid c$, then $a \mid c$.
    \item If $a \mid b$ and $c \mid d$, then $ac \mid bd$.
    \item If $a \mid b$, then $ac \mid bc$ for every $c$.
    \item If $ac \mid bc$ and $c \neq 0$, then $a \mid b$.
    \item If $a \mid b_i$ for some finite indices $i$, then $a \mid \sum_i c_i b_i$ for every $c_i$.
\end{enumerate}

If $\varepsilon \mid a$ for every $a$, then we call $\varepsilon$ a \emph{unit element}.
The unit elements of $\Z$ are $\pm 1$.

If $a \mid b$ and $b \mid a$ and $a \neq b$, then we call $a$ and $b$ \emph{associated elements}.
Two elements $a$ and $b$ are associated if and only if $a=\varepsilon b$ for some unit element $\varepsilon$.
Consequently, $a$ and $b$ are associated integers if and only if $|a| = |b|$.

Let $p \neq 0$ be a non-unit element.
We say that $p$ is an \emph{irreducible element} if $p=ab$ implies that $a$ or $b$ is an associated element of $p$ (and the other is a unit).
If an element is not irreducible, then it is \emph{composite}.
We call $p$ a \emph{prime element} if $p \mid ab$ implies that $p \mid a$ or $p \mid b$.
If $p$ is an irreducible element, then it is also a prime element.
In case of integers, the reverse is also true, i.e.\ every prime element is irreducible.

\begin{theorem}[The Fundamental Theorem of Arithmetic]
    \label{th:fta}
    If $a \neq 0$ is not a unit element, then it is a product of irreducible elements.
    The product is unique (up to ordering and up to multiplication with unit elements).
\end{theorem}

If $1 < n \in \Z$, then the \emph{canonical form} of
\[
    n = p_1^{\alpha_1} \cdots \ p_r^{\alpha_r}
\]
where $p_i$'s are different prime numbers (positive prime elements of $\Z$) and $\alpha_i > 0$ for all $i=1,\ldots,r$.

\begin{theorem}[Euclid]
    There are infinitely many prime numbers.
\end{theorem}
\begin{proof}
    Let us assume that there are only finite many primes $p_1,\ldots,p_n$.
    In this case the long division of $n = p_1 \cdots p_n + 1$ with $p_i$ yields a remainder of $1$ for every prime.
    This means that $n$ does not have canonical form, which contradicts Theorem \ref{th:fta}.
\end{proof}

\begin{theorem}[Distribution of prime numbers]
    The following statements illustrate some properties of the distribution of prime numbers:

    \begin{enumerate}
        \item If $N > 1$, then $\exists a > 2$ such that $a + 1, a + 2, \ldots, a+N$ are all composite numbers.
        \item For every $M > 2$, there is a prime number between $M$ and $2M$.
    \end{enumerate}
\end{theorem}

Let $\pi(x)$ denote the number of positive prime numbers below $x$.
\begin{theorem}[Prime Number Theorem]
    An approximation of $\pi(x)$ is $\frac{x}{\ln x}$. In other words
    \[
        \lim_{x \to +\infty} \frac{\pi(x)}{x/\ln x} = 1.
    \]
\end{theorem}

\subsection{Greatest common divisor}

The \emph{greatest common divisor} of $a_1,\ldots,a_n$ elements is $d$, if
\begin{itemize}
    \item $d \mid a_i$ for all $i$, i.e.\ $d$ is a common divisor of the elements and
    \item if $d' \mid a_i$ for all $i$, then $d' \mid d$ that is $d$ is maximal with respect to divisibility.
\end{itemize}
From the definition, it is clear that if $d$ is a greatest common divisor and $\varepsilon$ is a unit element, then $\varepsilon d$ is also a greatest common divisor.
We usually fix a greatest common divisor for integers by nominating the positive one.
We will use the notation $\gcd \left(a_1,\ldots,a_n \right)$.

From the definition we can see that
\begin{enumerate}
    \item $\gcd(a,0)=a$,
    \item $\gcd(0,0)=0$,
    \item $\gcd(a,b)=\gcd(b,a)$,
    \item $\gcd(a, b) = \gcd(a - b, b)$ or in general $\gcd(a-qb, b) = \gcd(a,b)$ for any $q$. Specifically $\gcd(a \bmod b, b) = \gcd(a, b)$.
\end{enumerate}

From the last property we have that
\begin{align*}
    \gcd(a, b) &= \gcd(a \bmod b, b) \\
               &= \gcd\left(a \bmod b, b \bmod \left(a \bmod b\right)\right) \\
               &= \gcd\left(\left(a \bmod b\right) \bmod b \bmod \left(a \bmod b\right), b \bmod \left(a \bmod b\right)\right).
\end{align*}
Let us define the recurrence relation $r_0 = a$, $r_1 = b$ and $r_{n+2} = r_{n} \bmod r_{n+1}$.
In this case the equations above become
\begin{align*}
    \gcd(r_0, r_1) &= \gcd(r_2, r_1) \\
                   &= \gcd\left(r_2, r_3\right) \\
                   &= \gcd\left(r_4, r_3\right).
\end{align*}
We can swap the arguments of $\gcd$, so
\begin{align*}
    \gcd(r_0, r_1) &= \gcd(r_1, r_2) \\
                   &= \gcd\left(r_2, r_3\right) \\
                   &= \gcd\left(r_3, r_4\right)
\end{align*}
The sequence in strictly decreasing, that is $r_n > r_{n+1}$ and $r_{n} \ge 0$ from $n=2$.
This means that there is an index $N$ such that $r_N = 0$, but $r_{N-1} > 0$.
From the properties and definition of recurrence relation it is clear, that
\[
    \gcd(a, b) = \gcd(r_{N-1}, 0) = r_{N-1}.
\]

The argument above gives the \emph{Euclidean algorithm} in recursive form:
\begin{algorithm}
    \caption{Recursive Euclidean algorithm}
    \label{alg:recursive-gcd}
    \begin{algorithmic}[1]
        \Procedure{GCD}{$a$, $b$}
            \If{$b = 0$}
                \State \textbf{return} a
            \EndIf
            \State \textbf{return} \Call{GCD}{$b$, $a \bmod b$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

We can transform the recursion into a loop.
At any given step, we only need $r_{n}$ and $r_{n+1}$ to produce $r_{n+2}$.
\begin{algorithm}
    \caption{Iterative Euclidean algorithm}
    \label{alg:iterative-gcd}
    \begin{algorithmic}[1]
        \Procedure{GCD}{$a$, $b$}
            \State $r_{\mathrm{old}},\ r_{\mathrm{new}} \gets a,\ b$
            \While{$r_{\mathrm{new}} \neq 0$}
                \State $r_{\mathrm{old}},\ r_{\mathrm{new}} \gets r_{\mathrm{new}},\ r_{\mathrm{old}} \bmod r_{\mathrm{new}}$
            \EndWhile
            \State \textbf{return} $r_{\mathrm{old}}$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

For later applications we not only need the $\gcd(a, b) = d$ but two additional elements $x$ and $y$, such that
\[
    d = ax + by.
\]
We can extend Euclidean algorithm to calculate $x$ and $y$.
For this, we are going to ensure that during the algorithm we are updating producing $x_n$ and $y_n$ for each $r_n$, such that
\[
    r_n = ax_n + by_n.
\]
This will be our loop invariant property, i.e.\ this property will be true for $n$ and $n+1$ whenever we enter the body of the loop and we will make sure that it is true for $n+2$ after the last statement of the body.
Before the first execution of the loop body, the values $x_0 = 1$, $y_0 = 0$, $x_1 = 0$ and $y_0 = 1$ will suffice.
Let $q = \left\lfloor r_n / r_{n+1} \right\rfloor$. During the body of the loop we have that
\begin{align*}
    r_{n+2} &= r_n \bmod r_{n+1} \\
            &= r_n -  q r_{n+1} \\
            &= \left(a x_n + b y_n\right) -q\left(a x_{n+1} + b y_{n+1}\right) \\
            &= a\left(x_n - q x_{n+1}\right) + b\left(y_n - q y_{n+1}\right),
\end{align*}
so $x_{n+2} = x_n - q x_{n+1}$ and $y_{n+2} = y_n - q y_{n+1}$ will work.
Again, we only need tha last two value of $x$ and $y$ during the calculations.
We call the procedure \emph{Extended Euclidean algorithm}:
\begin{algorithm}
    \caption{Extended Euclidean algorithm}
    \label{alg:extended-gcd}
    \begin{algorithmic}[1]
        \Procedure{GCD}{$a$, $b$}
            \State $r_{\mathrm{old}},\ x_{\mathrm{old}},\ y_{\mathrm{old}} \gets a,\ 1,\ 0$
            \State $r_{\mathrm{new}},\ x_{\mathrm{new}},\ y_{\mathrm{new}} \gets b,\ 0,\ 1$
            \While{$r_{\mathrm{new}} \neq 0$}
                \State $r_{\mathrm{old}},\ q,\ r_{\mathrm{new}} \gets r_{\mathrm{new}},\ \divmod{r_{\mathrm{old}}}{r_{\mathrm{new}}}$
                \State $x_{\mathrm{old}},\ x_{\mathrm{new}} \gets x_{\mathrm{new}},\ x_{\mathrm{old}} - q x_{\mathrm{new}}$
                \State $y_{\mathrm{old}},\ y_{\mathrm{new}} \gets y_{\mathrm{new}},\ y_{\mathrm{old}} - q y_{\mathrm{new}}$
            \EndWhile
            \State \textbf{return} $r_{\mathrm{old}},\ x_{\mathrm{old}},\ y_{\mathrm{old}}$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsection{Linear Diophantine equation}

Let $a,b,c \in \Z$ and we search for the integers solutions $x,y \in Z$ for the equation
\[
    ax + by = c.
\]

If $d = \gcd(a, b)$ then $d \mid ax + by$ there $d \mid c$. Otherwise there is no solution.
With Extended Euclidean algorithm, we can compute $d = ax' + by'$.
Since $d \mid c$, there exists $m \in \Z$ for which $c = d \cdot m$.
In summary
\[
    a \left(x'm\right) + \left(x'm\right) = d\cdot m = c,
\]
so $x_0 = x'm$ and $y_0 = y'm$ is a pair of solution.

If there is a pair of solution, then there are infinitely many solutions in the form of
\[
    x = x_0 + k\frac{b}{d}, \quad y = y_0 - k\frac{a}{d}
\]
where $k \in Z$ and there are no other solutions.

\subsection{Modular arithmetic}

Let $m > 1$. We say that $a$ is \emph{congruent to} $b$ $\pmod{m}$, if $m \mid a - b$.
The notation for this relation $a \equiv b \pmod{m}$.
Otherwise we say, that $a$ is \emph{incongruent to} $b$, or $a \not\equiv b \pmod{m}$.

For a fixed $m > 1$, $a \equiv b \pmod{m}$ defines an equivalence relation over the $\Z$.
The equivalence classes are called \emph{residue class}es.
The residue class of $a$ is
\[
    \overline{a} = \left\{ a + km : k \in \Z \right\}.
\]
Let us denote
\[
    \Z_m = \left\{ \overline{0}, \overline{1}, \ldots, \overline{m-1} \right\}
\]
the set of residue classes.

\begin{theorem}[The compability of integer operations with the residue classes]
    \label{th:compability}
    If $a \equiv b \pmod{m}$ and $c \equiv d \pmod{m}$, then
    \begin{itemize}
        \item $a + c \equiv b + d \pmod{m}$ and
        \item $ac \equiv bd \pmod{m}$.
    \end{itemize}

    This can be stated on residue class level as well:
    \begin{enumerate}
        \item $\overline{a} + \overline{b} = \left\{ a' + b' : a' \in \overline{a}, b' \in \overline{b} \right\} = \overline{a+b}$ and
        \item $\overline{a} \cdot \overline{b} = \left\{ a'b' : a' \in \overline{a}, b' \in \overline{b} \right\} = \overline{ab}$.
    \end{enumerate}
\end{theorem}

Theorem \ref{th:compability} enables us to work on the \emph{residue system} level, i.e. on
\[
    \Z_m = \left\{ \overline{0}, \overline{1}, \ldots, \overline{m-1} \right\} \simeq \left\{ 0, 1, \ldots, m-1 \right\}
\]
The operations on the residue system can be defined as
\[
    a +_m b = (a + b) \bmod m, \quad a \cdot_m b = ab \bmod m.
\]

\begin{theorem}[Structure of $\Z_m$]
    Let $m > 1$ and $a \neq 0$.
    \begin{enumerate}
        \item If $\gcd(a, m) \neq 1$, then $ax \equiv 0 \pmod{m}$ has non-zero solution.
        \item If $\gcd(a, m) = 1$, then $ax \equiv 1 \pmod{m}$ has a solution.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item
            If $d = \gcd(a, m) > 1$, then
            \[
                \frac{a}{d}m = a\frac{m}{d} \equiv 0 \pmod{m},
            \]
            so $x = m / d$ is nonzero solution.
        \item
            With the extended Euclidean algorithm, we can compute $1 = ax + my$.
            Rearranging this, we have that $ax - 1 = -ym$. So $m \mid ax - 1$, which is by definition means that $ax \equiv 1 \pmod{m}$.
    \end{enumerate}
\end{proof}
If $ax \equiv 1 \pmod{m}$, then we call $x$ the \emph{modular inverse} of $a$, denoted by $a^{-1}\pmod{m}$.

\subsection{Solving linear congruence equations}

We search for the integer solutions of $ax \equiv b \pmod{m}$.
We can convert this expression into the linear Diophantine equation $ax+my=b$.
We know that $d = \gcd(a,m) \mid b$, that is $b = s \cdot d$ otherwise there is no solution.

A particular $x_0$ can be found by extended Euclidean algorithm.
If $d = ax'+my'$, then $x_0=x's$ and the general solutions have the form of $x = x_0 + k\frac{m}{d}$ for integers $k$.

We know that $x \equiv x + m \pmod{m}$ and for every $1 < n < m$, $x \not \equiv x+n \pmod{m}$.
This means if $1 < k\frac{m}{d} < m$, then $x \not\equiv x+k\frac{m}{d} \pmod{m}$.
So we can list all the different solutions $\pmod{m}$ with $k=0,\ldots,d-1$.

\subsection{Solving systems of linear congruence relations}

No we are searching for common integer solution of the following two linear congruence equations
\[
    z \equiv a \pmod{c}, \quad z \equiv b \pmod{d},
\]
where $a, b \in \Z$ and $0 < c,d \in Z$.

\begin{theorem}[Chinese Remainder Theorem (CRT)]
    If $\gcd(c,d)=1$, then the system above has a solution and the solution is unique $\pmod{cd}$.
\end{theorem}

\begin{proof}
    We only prove the existence.

    With extended Euclidean algorithm we can compute $cx + dy = 1$.
    If we let $z = bcx + ady$, then
    \[
        bcx + ady \equiv 0 + ady \equiv ady \equiv a \pmod{c},
    \]
    since $c \mid bcx$ and $y=d^{-1} \pmod{c}$.
    The other congruence relation can proven in the same way.
\end{proof}

We can extend this method for solving more than two equations by replacing two of them with the common solution calculated with the help of previous theorem.

\subsection{Euler's totient function}

For $0 < n \in \Z$ we can define \emph{Euler's totient function} as
\[
    \varphi(n)  = \left \lvert \left \{ a \in \Z : 1 \le a \le n, \gcd(a,n) = 1 \right \} \right \rvert.
\]

If $p$ is a positive prime number, then $\varphi(p) = p - 1$.

If $1 < k \in Z$, then $\gcd(a, p^k) > 1$ means that $p \mid a$ for integers $1 \le a \le p^k$.
This means that $a = p, 2p, \dots, p^{k-1}p$. Due to this reasoning, we have that
\[
    \varphi\left( p^k \right) = p^k - p^{k-1} = p^k \left( 1 - \frac{1}{p} \right).
\]

If $\gcd(n,m)=1$, then $\varphi(nm) = \varphi(n)\varphi(m)$. This last statement is the consequence of CRT.

In summary, if the canonical form of $1 < n = p_1^{\alpha_1}\cdots p_r^{\alpha_r}$, then
\[
    \varphi(n) = n \prod_{1}^{r} \left( 1 - \frac{1}{p_i} \right).
\]

\begin{theorem}[Euler-Fermat]
    If $1 < n \in Z$ and $gcd(a,n) = 1$ for $a \in Z$, then $a^{\varphi(n)} \equiv 1 \pmod{n}$.
\end{theorem}

\begin{theorem}[Fermat's little theorem]
    If $1 < p \in Z$ is a prime number and $a \in Z$, then $a^{p} \equiv a \pmod{p}$. If in addition $p \nmid a$, then $a^{p-1} \equiv 1 \pmod{p}$.
\end{theorem}

\subsection{Probabilistic primality testing}

If $p$ is a prime number, then for any $1 < a < n-1$, due to Euler-Fermat Theorem $a^{p-1} \equiv 1 \pmod{p}$.
This statement is known as \emph{Fermat's little theorem} and we can use it as primality testing for $n > 2$ odd integer:
\begin{enumerate}
    \item Select randomly $a \in_R \left \{ 2, \ldots, n-2 \right \}$.
    \item If $\gcd(a,n)=1$ and $a^{n-1} \equiv 1 \pmod{n}$, then $n$ is probably a prime number. Otherwise it is composite.
\end{enumerate}

There are examples of such composite numbers $n$ that for every base $a$, where $\gcd(a,n)=1$ the property $a^{n-1} \equiv 1 \pmod{n}$ still holds.
These are called \emph{Carmichael numbers} and there are infinitely many of them.

We can use additional properties for further testing.

\begin{theorem}
    A positive integer $p$ is a prime number if and only if the only solutions of $x^2 \equiv \pmod{p}$ are $x \equiv \pm 1 \pmod{m}$.
    On the residue system level this means that $p$ is a prime number if and only if in $\Z_p$ the square roots of $1$ are $1$ and $p-1$.
\end{theorem}

Let $n-1 = 2^s q$, where $q$ is odd. Combining Fermat's little theorem and the previous property of prime numbers, the sequence
\[
    a^{2^s q} \mod n, a^{2^{s-1} q} \mod n, \ldots, a^q \mod n
\]
must start with at least one 1 and if there are any elements not equal to 1, the first such element must be $n-1$.

Based on these arguments we arrive the Miller-Rabin primality testing algorithm.
The probability of false positive response $p < \frac{1}{4}$. If we repeat the trial $k$ times independently, then $p < \frac{1}{4^k}$.

\begin{algorithm}
    \caption{Miller-Rabin probabilistic primality test}
    \label{alg:miller-rabin}
    \begin{algorithmic}[1]
        \Procedure{MillerRabin}{$n$}
            \State Compute integers $s, q$ such that $n-1=2^s q$ and $q$ is odd.
            \State Select $a \in_R \left \{ 2,\ldots,n-2 \right \}$.
            \State $x \gets a^q \mod n$ \Comment{Use fast modular exponentiation}
            \If{$x=1$}
                \State \textbf{return} probably prime
            \EndIf
            \For{$k=0,\ldots,s-1$}
                \If{$x = n - 1$}
                    \State \textbf{return} probably prime
                \EndIf
                \State $x \gets x^2 \mod n$.
            \EndFor
            \State \textbf{return} composite
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsection{Discrete logarithm problem}

Let us fix a prime number $p$. We will denote the nonzero elements of $\Z_p$ with $\Z_p^*$.
The \emph{(multiplicative) order} of $a \in \Z_p^*$, denoted by $\mathrm{ord}_p(a)$ is defined by
\[
    \mathrm{ord}_p(a) = \min\left\{ 0 < n \in \Z : a^{n} \mod p = 1 \right\}.
\]

Due to Fermat's little theorem ($a^{p-1} \mod p = 1$), the order of $a$ always exists and $\mathrm{ord}_p(a) \le p-1$.
If $a^i \mod p = a^j \mod p$ with $i > j$, then $a^{i-j} \mod 1$. This means $\mathrm{ord}_p(a) \le i-j$.
From this, we can see that if $\mathrm{ord}_p(a) = n$, then the elements $a, a^2, \ldots, a^n$ are all different.
If the order the element $g$ is maximal, i.e. $\mathrm{ord}_p(g) = p - 1$, then $\Z_p^* = \left\{ g, g^2, \ldots, g^{p-1} \right\}$.
In this case we call $g$ a \emph{generator} element of $Z_p^*$.

Let $g$ be a generator element of $\Z_p^*$, $k \mid p-1$ with $0 < k < p-1$ and $p-1 = kn$ for some $n$ integer.
In this case $g^{p-1} \mod p = (g^k)^n \mod p = 1$ and $g^k \mod p \neq 1$.
If $s \mid k$, then $s \mid p-1$, so $g^s \mod p \neq 1$ as well.

This reasoning gives us the following way of testing whether an element $g$ is generator or not:
If $k$ is a maximal proper divisor of $p-1$ ($k$ does not divide any other factor of $p-1$), then $g^k \mod p \neq 1$.
If the canonical form of $p-1 = p_1^{\alpha_1}\cdots p_r^{\alpha_r}$, then the maximal divisors of $p-1$ are
\[
    \frac{p-1}{p_i} \qquad i=1,\ldots,r.
\]

We call $p$ a \emph{Sophie Germain prime}, if it is a prime number and $p=2q+1$, where $q$ is also a prime number.
In this case the it is enough to test $g^2 \mod p \neq 1$ and $g^{q} \mod p \neq 1$.

Let $b \in \Z_p^*$, $0 \le k \le p-1$ and $a = b^k \mod p$.
In this case the \emph{discrete logarithm} or \emph{index} of $a$ in base $b$ in $\Z_p^*$ is $k$.
The notation is $\log_b a = k$ or $\mathrm{ind}_b a = k \pmod{p}$.

The discrete logarithm problem (DLP) is to find $k$ from $b$ and $a$.
Currently there is no efficient algorithm known to compute $k$.

\subsubsection{Diffie-Hellman key exchange}

Let us fix a large Sophie Germain prime number $p$ and search for a generator element $g$.
Alice secretly selects $1 \le a \le p-1$ and Bob does the same with $1 \le b \le p-1$.
Alice computes $A = g^a \mod p$ and Bob computes $B = g^b \pmod p$.
They share $A$ and $B$ between each other.
Alice computes $B^a \pmod p$, Bob computes $A^b \pmod p$.
Since
\[
    B^a = (g^b)^a = g^{ba} = g^{ab} = (g^a)^b = A^b,
\]
they have common secret key.
The security of the key exchange depends on that we assume DLP cannot be solved efficiently.

\section{RSA}

Let us fix two large prime numbers, $p$ and $q$ and calculate their product $n=pq$.
Also search for such integers $e$ and $d$ such that $ed \equiv 1 \mod \varphi(n)$, i.e. $ed = k(p-1)(q-1) + 1$ for some $k \in Z$.
We can make $n$ and $e$ public and keep $p$, $q$ and $d$ private.
If $0 < m < n$ and $c = m^e \mod n$, then
\[
    c^d = (m^e)^d = m^{ed} = m^{k(p-1)(q-1)+1} = (m^{p-1})^{k(q-1)}m \equiv m \pmod{p},
\]
since $m^{p-1} \equiv 1 \pmod{p}$. By the same argument we have that $m \equiv c^d \pmod{q}$.
We can solve this system with the Chinese remainder theorem and the solution is $m = c^d \mod n$.

Let $d_p = d \mod (p-1)$ and $d = k(p-1) + d_p$. In this case
\[
    c^d = c^{k(p-1)+d_p} = (c^{p-1})^k c^{d_p} \equiv c^{d_p} \pmod{p}.
\]
Also if $d_q = d \mod (q-1)$, then $c^d \equiv c^{d_q} \pmod{q}$.
Let $m_1 = c^{d_p} \mod p$ and $m_2 = c^{d_q} \pmod q$.
In this case if $m$ solves the system
\[
    m \equiv m_1 \pmod{p}, \qquad m \equiv m_2 \pmod{q},
\]
then $m$ solves the original problem.

Let us search for the solution tin the form of $m=m_2 + hq$.
In this case $m \equiv m_2 \pmod{q}$.
If $m_2 + hq \equiv m_1 \pmod{m}$, then $hq \equiv m_1 - m_2 \pmod{p}$.
Let us denote $q_{\mathrm{inv}}=q^{-1} \pmod{p}$.
In this case with $h = q_{\mathrm{inv}}(m_1 - m_2) \mod p$, then $m$ solves the system.

The security of RSA stands on the assumption that the factorization of $n$ into $p$ and $q$ cannot be done efficiently.
Due to this fact calculating $\varphi(n)$ without knowing $p$ and $q$ is hard as well.
This means that even by knowing $e$, it is hard to compute $d$.

We can use RSA to sign messages digitally.
If $\sigma = m^d \pmod n$, then we can check the signature of the the $m, \sigma$ pair by checking that $m \equiv \sigma^e \pmod{n}$.


\section{Polynomials over Fields}

Our main requirement from a field $F$ is that the linear equation $ax=b$ with $a, b \in F$, $a\neq 0$ has a solution $x \in F$.
We calculate the solution by cancelling $a$, i.e. $x=a^{-1}b$, where $a^{-1}$ is the multiplicative inverse of $a$.

The main examples of fields are $\mathbb{C}, \mathbb{R}, \mathbb{Q}$ and $\Z_p$ for prime $p$.

Informally, we can think of a polynomial as an expression in the form of
\[
    f = f_0 + f_1 x + f_2 x^2 + \cdots + f_n x^n.
\]
Here $f$ is the \emph{polynomial}, $f_i$ is a \emph{coefficient}, $f_i x^i$ is a \emph{term}, $f_0$ term is the \emph{constant term} and $f_n$ is the \emph{main coefficient}.
We call $x$ the indeterminate variable. If the coefficients are all from the field $F$, then $f$ is a polynomial over the field $F$.
The set of all polynomials over $F$ is denoted by $F[x]$. 
Sometimes we consider the polynomials as infinite sums of terms, where the coefficients are zero after $f_n \neq 0$, i.e.
\[
    f = f_0 + f_1 x + f_2 x^2 + \cdots + f_n x^n + 0x^{n+1} + 0x^{n+1} + \cdots
\]

Since we can map every $F \ni c \mapsto c + 0x + 0x^s + \cdots$, every constant is a polynomial also.
We can write $x = 0 + 1x + 0x^2 + \cdots$, therefore $x \in F[x]$.

The \emph{degree} of $f$ is defined by
\[
    \mathrm{deg}(f) = \begin{cases}
        -1, & f = 0 \\
        0, & f = c \in F, c \neq 0 \\
        n, & f_n \neq 0, f_{n+k} = 0, 0 < k \in \Z.
    \end{cases}
\]
In some sense, we can say that if the degree of polynomial is greater than the degree of another, then the first one is "larger".

\subsection{Operations of $F[x]$}

We work with polynomials as expression, we can add and multiply them.
Since we can identify a polynomial with its list of coefficients, we must determine the coefficients of the result of addition and multiplication from the coefficients of the operands.

Let 
\[
    f = f_0 + f_1 x + \cdots + f_n x^n,
\]
and
\[
    g = g_0 + g_1 x + \cdots + g_m x^m + 0x^{m+1} + \cdots 0x^n.
\]

\paragraph{Addition}
After collecting the term $h_k x^k$ with $h=f+k$, we have that $h_k x^k = f_k x^k + g_k x^k$, so $h_k = f_k + g_k$.
For the degree of $f+g$ we have that $\mathrm{deg}(f+g) \le \max \{ \mathrm{deg}(f), \mathrm{deg}(g) \}$.

\paragraph{Multiplication}
Again, we collect the term $h_k x^k$ with $h = fg$. We have that
\[
    h_k x^k = f_0 g_k x^k + f_1 x g_{k-1} x^{k-1} + \cdots + f_k x^k g_0,
\]
so
\[
    h_k = \sum_{i=0}^k f_i g_{k-i} = \sum_{j=0}^k f_{k-j} g_j = \sum_{i+j=k} f_j g_j.
\]
The degree of $fg$ is $\mathrm{deg}(f) + \mathrm{deg}(g)$.

\begin{theorem}[Euclidean divison in of polynomials over fields]
    If $f, g \in F[x]$ with $g \neq 0$, then there exists a unique pair $q,r \in F[x]$ such that $f = qg + r$ with $\mathrm{deg}(r) < \mathrm{deg}(g)$.
\end{theorem}
\begin{proof}
    We only prove the existence. Let $n = \mathrm{deg}(f)$ and $m = \mathrm{deg}(g)$. If $n < m$, then $q=0$, $r=f$ works.
    
    Otherwise for the polynomial $f^* = f - f_n g_m^{-1}x^{n-m} g$ we have that $\mathrm{deg}(f^*) < \mathrm{deg}(f)$.
    We repeat this process until $r = \mathrm{deg}(f^*) < m$ and record $f_n g_m^{-1}x^{n-m}$ into $q$.
\end{proof}

\subsection{Error detection with polynomials over $\Z_2$}

We want to send the bits $b_0 b_1 \ldots b_{k-1}$ over a noisy channel.
We assume that we receive the same number of bits, but the values might change.
On the receiver's end we want to be able to detect any error in communication.
For this reason we introduce redundant information $r_0 r_1 \ldots r_{m-1}$ into our bits.

The simples case is introducing only one bit.
Let
\[
    r_0 = \sum_{i=0}^{k-1} b_i \mod 2.
\]
If we send $r_0 b_1 \ldots b_{k-1}$, then we always send even number of bits that are set to $1$.
We reject the received message if there are odd number of bits to to $1$ in it.
This method is called \emph{parity check}, $r_0$ is the \emph{parity bit}.
This simplest form of error detection method is able to detect all such errors where odd number of bits are flipped.
But it is incapable to detect even number of errors.

We can think of the bits $b_0 b_1 \dots b_{k-1}$ as the coefficients of the polynomial
\[
    b = b_0 + b_1 x + \cdots b_{k-1} x^{k-1} \in \Z_2[x].
\]
If we denote $r_0 r_1 \ldots r_{m-1} b_0 \ldots b_{k-1}$ with $r||b$, then
\[
    r||b = r_0 + r_1 x + \cdots r_{m-1} x^{m-1} b_0 x^{m} + \cdots b_{k-1} x^{k-1 + m} = r + bx^m.
\]

If $g \in \Z_2[x]$ with $\mathrm{deg}(g) = m$, then we can write $b x^m = qg + r$ where $\mathrm{deg}(r) < m$.
From this we, have $b x^m - r = qg$, so $g \mid b x^m - r$. But in $\Z_2[x]$, the polynomials $b x^m - r$ and $b x^m + r$ are the same.
If we send the coefficients of $b x^m + r$, then on the receiver's end we can check if $g \mid b x^m + r$ still stands.

This error detection method is called \emph{Cyclic redundancy check} or \emph{CRR} and $g$ is a \emph{CRC polynomial}.

\subsubsection{Designing CRC polynomials}

We can think of the error of the transmission $e$ as a polynomial over $\Z_2$.
So we receive $bx^m + r + e$. To be able to detect the error, we must design $g$ such that $g \nmid e$.
Some things to consider:
\begin{itemize}
    \item The degree of $g$ influences the error detection capabilities, for e.g. the length of messages that can be sent in one go.
    \item In case of one bit error, we have that $e=x^s$. If $g$ has at least two bits set, i.e. $x^i + x^j \mid g$, then $g \nmid x^s$.
    \item Two bit errors can written as $x^i + x^j = x^i(1 + x^j)$. We have already covered the $x^i$ term.
          To protect against $(1 + x^j)$ we should make $1 + x^k \mid g$ with as big $k$ as possible.
    \item The CRC polynomial $x+1$ is the parity check method. If $x + 1 \mid g$, then CRC detects all odd number of errors.
    \item All burst error of $n$ bits is detected a polynomial with $g_0 = 1$ and $\mathrm{deg}(g) \ge n$.
\end{itemize}

\subsection{Number Theory of $F[x]$}

$F[x]$ behaves just like $\Z$ so we can define divisibility.
The units of $F[x]$ are the nonzero constant polynomials, since $F[x] \ni f = cg$ with $g_k = f_k c^{-1}$.
The associated elements of $f$ are $cf$ for $0 \neq c \in F$.
The irreducible and prime elements of $F[x]$ are the same. The fundamental theorem of arithmetic can be stated for polynomials as well.
We can write every polynomial of $F[x]$ as uniquely as product of irreducible polynomials, i.e. 
\[
    f = f_n\left(g^{(1)}\right)^{\alpha_1}\cdots \left(g^{(r)}\right)^{\alpha_r},
\]
where $g^{(i)} \in F[x]$ is irreducible, with main coefficient of $1$, $0 < \alpha_i \in \Z$ for $i=1,\ldots,r$.

Polynomials have greatest common divisor, we can distinguish one by taking the one with main coefficient of $1$.
Extended Euclidean algorithms works for polynomials.

We can define congruence relation with a fixed modulus non-constant $m \in F[x]$.
We can distinguish a unique element from every residue class with degree less than $\mathrm{deg}(m)$.
These elements form the residue system $F[x]_m$.

The structure of $F[x]_m$ is the same as with $\Z_m$.
If $\gcd(a,n)=1$, then $a$ has modular inverse which can be computed with extended Euclidean algorithm.

If $m$ is irreducible, then every non-zero element in $F[x]_m$ has a modular inverse, so $F[x]_m$ is a field.

\subsection{Roots of Polynomials}

If $f \in F[x]$ and $r \in F$ then we can define the mapping $r \mapsto f(r) = \sum_{i=0}^n f_i r^i$.
This is the \emph{polynomial function} defined by the polynomial $f$.
We denote the function by $\hat{f}$.

Note that $x, x^p \in \Z_p[x]$ are different polynomials, but $r^p \equiv r \pmod{p}$ so they share the same polynomial function.

If for $c \in F$ we have that $f(c) = 0$, then we call $c$ the \emph{root} of $f$.

\begin{theorem}
    If $0 \neq f \in F[x]$, $c \in F$ and $f(c) = 0$, then $x-c \mid f$.
\end{theorem}
\begin{proof}
    With Euclidean division we can write $f=(x-c)q + r$.
    Since $\mathrm{deg}(r) < \mathrm{deg}(x-c) = 1$, we have that $r=c' \in F$.
    But $0 = f(c) = (c-c)q + c'$, so $c'=0$.
\end{proof}

This theorem has the following consequences:
\begin{enumerate}
    \item If $0 \neq f \in F[x]$, then $f$ has at most $\mathrm{deg}(f)$ roots.
    \item If the polynomial functions of $f,g \in F[x]$ with $\mathrm{deg}(f), \mathrm{deg}(g) \le n$ agree on $n+1$ points, then $f=g$. Otherwise $f-g$ would have $n+1$ roots.
    \item If $F$ is infinite, then two polynomials cannot share a polynomial function. Otherwise the difference polynomial would have infinite roots.
\end{enumerate}

If $0 \neq f \in F[x]$, $1 \le n \in \Z$. The $c$ is the root of $f$ \emph{multiplicity} of $n$ if $(x-c)^n \mid f$, but $(x-c)^{n+1} \nmid f$.

\begin{theorem}[Lagrange interpolation]
    If $c_0,c_1,\ldots,c_n \in F$ are different and $d_0,d_1,\ldots,d_n \in F$, then there is a unique $f\in F[x]$ such that $f(c_j) = d_j$ for $j=0,\ldots,n$.
    The $j$-th Lagrange base polynomial of the points $c_0,c_1,\ldots,c_n$ is defined by
    \[
        l_j = \frac{\prod_{i \neq j} (x - c_j)}{\prod_{i \neq j} (c_i - c_j)},
    \]
    and $f = \sum_{j=0}^n d_j l_j$.
\end{theorem}

\subsection{Irreducible polynomials}

\begin{theorem}[The Fundamental Theorem of Algebra]
    If $1 \le n \in \Z$, $f \in \mathbb{C}[x]$ with $\mathrm{deg}(f) = n$, then there exists $c\in\mathbb{C}$ such that $f(c) = 0$.
\end{theorem}

The irreducible polynomials of $\mathbb{C}[x]$ exactly the linear polynomials.

The irreducible polynomials of $\mathbb{R}[x]$ are the linear polynomials and such quadratic polynomials that do not have real root, for. e.g. $x^2 + 1$.

If $p$ is a prime number and $1 \le n \in \Z$, then $x^n - p \in \mathbb{Q}[x]$ is irreducible.

For every $1 \le n \in \Z$, there exists $f \in \Z_p[x]$ such that $f$ is irreducible.

\subsection{Field extensions}

Given and irreducible polynomial $m \in F[x]$, $F[x]_m$ is a field.

Let $F=\mathbb{R}$ and $m = x^2 + 1$. In this case the elements of the residue system are $ax+b$ for $a,b \in \mathbb{R}$.
Now the product of two elements $\pmod{x^2 + 1}$ is
\begin{align*}
    (ax + b)(cx + d) \mod x^2 + 1 = &= ac + (bc+ad)x + bdx^2 \mod x^2 + 1 \\
                                    &= ac + (bc+ad)x + bdx^2 - bdx^2 - bd \\
                                    &= (ac - bd) + (bc + ad)x.
\end{align*}
This means that $\mathbb{R}[x]_{x^2 + 1} \cong \mathbb{C}$.

For $\Z_p[x]_m$ with irreducible $m$ with degree of $n$, the residue systems consists of the elements
\[
    f = f_0 + f_1 x + f_2 x^2 + \cdots + f_{n-1} x^{n-1},
\]
with $f_i \in \Z_p$. So there are $p^n$ elements in $\Z_p[x]_m$. It does not matter which irreducible $m$ we select, they will yield the same fields.

All the finite fields are formed this way, we denote them by $\mathbb{F}_q = GF(q)$ with $q=p^n$.

\subsection{Shamair's secret sharing}

Let us assume that $2 \le k \le n$.
We want to share a secret between $n$ member, which they can open only when at least $k$ members are present.

Let us a fix a finite field $F=GF(q)$ with $q > n$ and $0 \neq \alpha \in F$ such that $\mathrm{ord}_q(\alpha) \ge n$.
The secret they want to share and cannot open without $k$ members of the party present is $s \in F$.
An orchestrator must select $f_i \in_{R} F$ for $i=1,\ldots,k-1$ and create the values $b_i = f(a_i)$ withe the polynomial
\[
    f = s + f_1 x + \cdots f_{k-1}x^{k-1}
\]
where $a_i = \alpha^{i}$ and $i=1,\ldots,n$. The secret is $s = f(0)$.
The orchestrator then distributes the the pairs $(a_i, b_i)$ between the members ($i=1,\ldots,n$).

If the
\[
    \left\{ i_1, i_2, \ldots, i_k \right\} \subset \left\{ 1,\dots,n \right\}
\]
members are present they can create the Lagrange interpolation polynomial $g$ for which $g(a_{i_j}) = b_{i_j}$.
Since $\mathrm{deg}(f), \mathrm{deg}(g) \le k-1$ and they agree on $k$ different points they must be the same polynomial, i.e.\ $f=g$.
The members present can recover the secret $s=g(0)$.

\subsection{Error correction with polynomials}

\subsubsection{Baby Reed-Solomon error correction}

The symbols of the message are coming from the finite field $F=GF(q)$.
The length of the message is $k$ and we add $m$ extra redundant symbols during encoding.
The length of the encoded message is $n=k+m$.
Select nonzero $\alpha \in F$ such that $\mathrm{ord}_q(\alpha) \ge n$.

\paragraph{Encoding}
The message $m=(m_0,\ldots,m_{k_1}) \in F^k$.
Let us create the Lagrange interpolation polynomial $g$ for which $g(\alpha^i) = m_i$ holds for $i=0,\ldots,k-1$.
The encoded message is 
\begin{align*}
    F \ni e &= (e_0,\ldots,e_{k-1},e_k,\ldots,e_{n-1}) \\
            &= (g(\alpha^0),\ldots,g(\alpha^{k-1}),g(\alpha^{k}),\ldots,g(\alpha^{n-1})) \\
            &= (m_0,\ldots,m_{k-1},g(\alpha^{k}),\ldots,g(\alpha^{n-1})).
\end{align*}

\paragraph{Decoding}
We receive $r=(r_0,\ldots,r_{n-1}) \in F^n$. For all $k$ element index set
\[
    \left\{ i_0,\ldots,i_{k-1} \right\} \subset \left\{ 0,\ldots,n-1 \right\},
\]
create the Lagrange interpolation polynomial $g$ with $g(\alpha^{i_j})=r_{i_j}$ for $j=0,\ldots,k-1$ and add one vote for this polynomial.

If there was no error in transmission, only one polynomial is created and it gets all the votes.
In this case, decoding is just trimming the redundancy symbols, i.e.\ $m_i=r_i$ for $i=0,\ldots,k-1$.

If there was at least one error, different polynomials will be created.
If there is a polynomial $g^*$ which has the majority of votes, we can decode with it.
Let $m_i = g^*(\alpha^i)$ for $i=0,\ldots,k-1$.
If we cannot select a winner, then we cannot correct the error and must report it.

\subsection{Grownup Reed-Solomon error correction}

Let $F=GF(q)$, $0 \neq \alpha \in F$ with $\mathrm{ord}_q(\alpha) \ge n$ and $0 < k < n$.
We want to detect and correct errors during the transmission of $m=m_0 + m_1 x + \cdots m_{k-1} x^{k-1}$.

\paragraph{Encoding}
Let us define the generator polynomial of the encoding with $g = \prod_{i=1}^t \left( x - \alpha^i \right)$ where $t=n-k$.
The encoded message is $c = mx^t - r$, where $r = mx^t \bmod g$.

\paragraph{Decoding}
Let us assume that we receive $c' = c + e$.
Since $g \mid c$, every root of $g$ must be a root of $c$ as well.
Let $s = s_0 + s_1x + \cdots s_{t-1}x^{t-1}$ where $s_i = c'(\alpha^{i+1})$.
If $e=0$, then $s=0$ and we can accept $c'$ as a transmission without any error.
We can extract the original message as the last $k$ coefficients of $c'$.

If $s \neq 0$, then there was error during transmission, so $e \neq 0$.
We can modify the extended Euclidean algorithm to produce the \emph{error location polynomial} $L$ and the \emph{error value polynomial} $E$ in Algorithm \ref{alg:calc-error-polynomials}.

\begin{algorithm}
    \caption{Calculating the error location and error value polynomials}
    \label{alg:calc-error-polynomials}
    \begin{algorithmic}[1]
        \Procedure{CalculateErrorPolynomails}{$s$, $t$}
            \State $r^{(\mathrm{old})},\ y^{(\mathrm{old})} \gets x^t,\ 0$
            \State $r^{(\mathrm{new})},\ y^{(\mathrm{new})} \gets s,\ 1$
            \While{$\mathrm{deg}(r^{(\mathrm{new})}) > \frac{t}{2}$}
                \State $r^{(\mathrm{old})},\ q,\ r^{(\mathrm{new})} \gets r^{(\mathrm{new})},\ \divmod{r^{(\mathrm{old})}}{r^{(\mathrm{new})}}$
                \State $y^{(\mathrm{old})},\ y^{(\mathrm{new})} \gets y^{(\mathrm{new})},\ y^{(\mathrm{old})} - q y^{(\mathrm{new})}$
            \EndWhile
            \State $L \gets y^{(\mathrm{new})}/y^{(\mathrm{new})}(0)$
            \State $E \gets r^{(\mathrm{new})}/y^{(\mathrm{new})}(0)$
            \State \textbf{return} $L,\ E$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

It can be proven that $L=\prod_{e_j \neq 0}\left( 1 - \alpha^{j}x \right)$. This means that if $\alpha^{-j}$ is a root of $L$, then $e_j \neq 0$, i.e.\ $j$ is an error position of $c'$.
For the other computed polynomial we have that $E = \sum_{e_j \neq 0} \alpha^{j} e_j L_j$ where 
\[
    L_j = \prod_{\substack{e_i \neq 0 \\ j \neq i}} \left( 1 -\alpha^i x\right).
\]
Therefore
\[
    e_j = \frac{E(\alpha^{-j})}{\alpha^{j}L_j(\alpha^{-j})}
\]
is the error at location $j$.
\end{document}